from translations import tr, get_language
from PIL import Image, UnidentifiedImageError
from multiprocessing import Pool, cpu_count
import signal
import sys
import os
import re
import time
import shutil
import psutil
import hashlib
import logging
from pathlib import Path

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('photo_tool.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
# LANG = 'zh'
# TEXTS = {
#     'zh': {
#         'log_config': 'æ—¥å¿—é…ç½®',
#         'hash_desc': 'è®¡ç®—å›¾ç‰‡æ–‡ä»¶çš„å“ˆå¸Œå€¼ï¼Œæ”¯æŒ md5/sha1ã€‚',
#         'img_collect': 'é€’å½’æ”¶é›†æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶è·¯å¾„ã€å¤§å°ã€å°ºå¯¸ã€‚',
#         'img_collect_ret': 'è¿”å›ï¼š[{path, size, shape}...]',
#         'img_found': 'å…±å‘ç°å›¾ç‰‡æ–‡ä»¶ {count} å¼ ',
#         'img_meta': 'æˆåŠŸè¯»å–å…ƒæ•°æ®å›¾ç‰‡æ•°: {count}',
#         'img_size_fail': 'æ— æ³•è¯»å–å›¾ç‰‡å°ºå¯¸, é”™è¯¯: {err}',
#         'img_size_fail2': 'æ— æ³•è¯»å–å›¾ç‰‡å°ºå¯¸: {path}, é”™è¯¯: {err}',
#         'file_size_fail': 'æ— æ³•è¯»å–æ–‡ä»¶å¤§å°: {path}, é”™è¯¯: {err}',
#         'vid_collect': 'é€’å½’æ”¶é›†æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰è§†é¢‘æ–‡ä»¶è·¯å¾„ã€å¤§å°ã€æ–‡ä»¶åã€‚',
#         'vid_collect_ret': 'è¿”å›ï¼š[{path, size, name}...]',
#         'vid_found': 'å…±å‘ç°è§†é¢‘æ–‡ä»¶ {count} ä¸ª',
#         'vid_meta': 'æˆåŠŸè¯»å–å…ƒæ•°æ®è§†é¢‘æ•°: {count}',
#         'vid_info_fail': 'æ— æ³•è¯»å–è§†é¢‘æ–‡ä»¶ä¿¡æ¯: {path}, é”™è¯¯: {err}',
#         'hash_fail': 'å“ˆå¸Œè®¡ç®—å¤±è´¥: {path}, é”™è¯¯: {err}',
#         'dedup_desc': 'å»é‡æ¨¡å¼ä¸»æµç¨‹ï¼šæŸ¥æ‰¾é‡å¤å›¾ç‰‡å’Œè§†é¢‘å¹¶è¾“å‡ºæŠ¥å‘Šã€‚',
#         'dry_run': '[DRY-RUN] å½“å‰ä¸ºåªè¯»æ¨¡å¼ï¼Œä¸ä¼šå¯¹ä»»ä½•æ–‡ä»¶åšå†™å…¥æ“ä½œã€‚',
#         'group_count': 'åˆ†ç»„åéœ€è¿›ä¸€æ­¥æ¯”å¯¹çš„ç»„æ•°: {count}',
#         'group_processing': 'æ­£åœ¨å¤„ç†åˆ†ç»„: å¤§å°={size}, å°ºå¯¸={shape}, æ–‡ä»¶æ•°={count}',
#         'dedup_done': 'å»é‡å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {path}',
#         'supp_desc': 'å¢è¡¥æ¨¡å¼ä¸»æµç¨‹ï¼šè¡¥å……å›¾ç‰‡å’Œè§†é¢‘å¹¶è¾“å‡ºæŠ¥å‘Šã€‚',
#         'main_img_count': 'ä¸»æ–‡ä»¶å¤¹å›¾ç‰‡æ•°: {main}, è¡¥å……æ–‡ä»¶å¤¹å›¾ç‰‡æ•°: {supp}',
#         'main_hash_done': 'ä¸»æ–‡ä»¶å¤¹å“ˆå¸Œé›†åˆæ„å»ºå®Œæˆï¼Œå”¯ä¸€å›¾ç‰‡æ•°: {count}',
#         'supp_dir': 'è¡¥å……å›¾ç‰‡_{timestamp}',
#         'supp_hash_fail': 'è¡¥å……å›¾ç‰‡å“ˆå¸Œå¤±è´¥ï¼Œè·³è¿‡: {path',
#         'supp_exists': 'å·²å­˜åœ¨ï¼Œæœªå¢è¡¥: {path}',
#         'disk_space': 'å¢è¡¥å›¾ç‰‡æ€»å¤§å°: {size:.2f} MB, ç›®æ ‡ç£ç›˜å‰©ä½™ç©ºé—´: {free:.2f} MB',
#         'disk_full': 'ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œæ“ä½œä¸­æ­¢ï¼',
#         'disk_full_exc': 'ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œæ— æ³•å®Œæˆå¢è¡¥æ“ä½œï¼',
#         'dry_run_supp': '[DRY-RUN] é¢„æ¼”å¢è¡¥: {src} -> {dst}',
#         'supp_copy': 'å¢è¡¥å›¾ç‰‡: {src} -> {dst}',
#         'supp_copy_fail': 'å¤åˆ¶å›¾ç‰‡å¤±è´¥: {src} -> {dst}, é”™è¯¯: {err}',
#         'vid_supp_exists': 'å·²å­˜åœ¨è§†é¢‘ï¼Œæœªå¢è¡¥: {path}',
#         'dry_run_vid': '[DRY-RUN] é¢„æ¼”å¢è¡¥è§†é¢‘: {src} -> {dst}',
#         'vid_supp_copy': 'å¢è¡¥è§†é¢‘: {src} -> {dst}',
#         'vid_supp_copy_fail': 'å¤åˆ¶è§†é¢‘å¤±è´¥: {src} -> {dst}, é”™è¯¯: {err}',
#         'dedup_img_group': 'é‡å¤å›¾ç‰‡ç»„{group_id} (å“ˆå¸Œ: {h}):',
#         'dedup_vid_header': '\né‡å¤è§†é¢‘æ–‡ä»¶ï¼š\n',
#         'dedup_vid_group': 'è§†é¢‘é‡å¤ç»„{idx}:',
#         'supp_report': 'å¢è¡¥å›¾ç‰‡æŠ¥å‘Š',
#         'supp_img_success': 'æˆåŠŸå¢è¡¥ {count} å¼ å›¾ç‰‡åˆ°: {dir}',
#         'supp_img_exists': 'å·²å­˜åœ¨ï¼ˆæœªå¢è¡¥ï¼‰{count} å¼ å›¾ç‰‡ï¼š',
#         'supp_vid_success': 'æˆåŠŸå¢è¡¥ {count} ä¸ªè§†é¢‘åˆ°: {dir}',
#         'supp_vid_exists': 'å·²å­˜åœ¨ï¼ˆæœªå¢è¡¥ï¼‰{count} ä¸ªè§†é¢‘ï¼š',
#         'en_dash': 'ï¼š',
#         'scanning_images': 'æ­£åœ¨æ‰«æå›¾ç‰‡æ–‡ä»¶...',
#         'images_found': 'å‘ç°å›¾ç‰‡æ–‡ä»¶ {count} å¼ ',
#         'scanning_videos': 'æ­£åœ¨æ‰«æè§†é¢‘æ–‡ä»¶...',
#         'videos_found': 'å‘ç°è§†é¢‘æ–‡ä»¶ {count} ä¸ª',
#         'analyzing_duplicates': 'æ­£åœ¨åˆ†æé‡å¤æ–‡ä»¶ï¼Œå…± {count} ç»„å¾…å¤„ç†...',
#         'analysis_complete': 'åˆ†æå®Œæˆ',
#     },
#     'en': {
#         'log_config': 'Log config',
#         'hash_desc': 'Calculate image file hash, supports md5/sha1.',
#         'img_collect': 'Recursively collect all image file paths, sizes, and shapes in folder.',
#         'img_collect_ret': 'Return: [{path, size, shape}...]',
#         'img_found': 'Found {count} image files',
#         'img_meta': 'Successfully read metadata for {count} images',
#         'img_size_fail': 'Failed to read image size, error: {err}',
#         'img_size_fail2': 'Failed to read image size: {path}, error: {err}',
#         'file_size_fail': 'Failed to read file size: {path}, error: {err}',
#         'vid_collect': 'Recursively collect all video file paths, sizes, and names in folder.',
#         'vid_collect_ret': 'Return: [{path, size, name}...]',
#         'vid_found': 'Found {count} video files',
#         'vid_meta': 'Successfully read metadata for {count} videos',
#         'vid_info_fail': 'Failed to read video file info: {path}, error: {err}',
#         'hash_fail': 'Hash calculation failed: {path}, error: {err}',
#         'dedup_desc': 'Deduplication main flow: find duplicate images/videos and output report.',
#         'dry_run': '[DRY-RUN] Readonly mode, no actual file operation.',
#         'group_count': 'Groups to further compare: {count}',
#         'group_processing': 'Processing group: size={size}, shape={shape}, count={count}',
#         'dedup_done': 'Deduplication done, report saved to: {path}',
#         'supp_desc': 'Supplement main flow: supplement images/videos and output report.',
#         'main_img_count': 'Main folder images: {main}, supplement folder images: {supp}',
#         'main_hash_done': 'Main folder hash set built, unique images: {count}',
#         'supp_dir': 'supplement_{timestamp}',
#         'supp_hash_fail': 'Supplement image hash failed, skip: {path}',
#         'supp_exists': 'Already exists, not supplemented: {path}',
#         'disk_space': 'Supplement images total size: {size:.2f} MB, target disk free: {free:.2f} MB',
#         'disk_full': 'Disk space not enough, abort!',
#         'disk_full_exc': 'Disk space not enough, cannot complete supplement!',
#         'dry_run_supp': '[DRY-RUN] Simulate supplement: {src} -> {dst}',
#         'supp_copy': 'Supplement image: {src} -> {dst}',
#         'supp_copy_fail': 'Copy image failed: {src} -> {dst}, error: {err}',
#         'vid_supp_exists': 'Video already exists, not supplemented: {path}',
#         'dry_run_vid': '[DRY-RUN] Simulate supplement video: {src} -> {dst}',
#         'vid_supp_copy': 'Supplement video: {src} -> {dst}',
#         'vid_supp_copy_fail': 'Copy video failed: {src} -> {dst}, error: {err}',
#         'dedup_img_group': 'Duplicate Image Group {group_id} (hash: {h}):',
#         'dedup_vid_header': '\nDuplicate Videos:\n',
#         'dedup_vid_group': 'Duplicate Video Group {idx}:',
#         'supp_report': 'Supplement Report',
#         'supp_img_success': 'Supplemented {count} images to: {dir}',
#         'supp_img_exists': 'Already exists (not supplemented) {count} images:',
#         'supp_vid_success': 'Supplemented {count} videos to: {dir}',
#         'supp_vid_exists': 'Already exists (not supplemented) {count} videos:',
#         'en_dash': ':',
#         'scanning_images': 'Scanning image files...',
#         'images_found': 'Found {count} image files',
#         'scanning_videos': 'Scanning video files...',
#         'videos_found': 'Found {count} video files',
#         'analyzing_duplicates': 'Analyzing duplicates, {count} groups to process...',
#         'analysis_complete': 'Analysis complete',
#     }
# }
# def get_text(lang, key, **kwargs):
#     s = TEXTS.get(lang, TEXTS['zh']).get(key, key)
#     return s.format(**kwargs) if kwargs else s
def get_optimal_chunk_size(file_size):
    """
    æ ¹æ®æ–‡ä»¶å¤§å°è®¡ç®—æœ€ä¼˜çš„è¯»å–å—å¤§å°
    """
    if file_size < 1024 * 1024:  # < 1MB
        return 8192  # 8KB
    elif file_size < 10 * 1024 * 1024:  # < 10MB
        return 65536  # 64KB
    elif file_size < 100 * 1024 * 1024:  # < 100MB
        return 262144  # 256KB
    else:  # >= 100MB
        return 1048576  # 1MB
    
def get_image_hash(image_path, method='md5', max_size=500*1024*1024):
    """
    æ”¹è¿›çš„å“ˆå¸Œè®¡ç®—å‡½æ•°ï¼Œä¼˜åŒ–å¤§æ–‡ä»¶å¤„ç†
    """
    try:
        normalized_path = normalize_path(image_path)
        
        if not safe_file_exists(normalized_path):
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            return None
        
        file_size = safe_file_size(normalized_path)
        if file_size == 0:
            logger.warning(f"æ–‡ä»¶ä¸ºç©º: {image_path}")
            return None
        
        if file_size > max_size:
            logger.warning(f"æ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡å“ˆå¸Œè®¡ç®—: {image_path} ({file_size/1024/1024:.1f}MB)")
            return None
        
        # æ ¹æ®æ–‡ä»¶å¤§å°é€‰æ‹©æœ€ä¼˜å—å¤§å°
        chunk_size = get_optimal_chunk_size(file_size)
        
        # é€‰æ‹©å“ˆå¸Œç®—æ³•
        if method == 'md5':
            hash_func = hashlib.md5()
        elif method == 'sha1':
            hash_func = hashlib.sha1()
        elif method == 'sha256':
            hash_func = hashlib.sha256()
        else:
            hash_func = hashlib.md5()
        
        bytes_processed = 0
        with open(normalized_path, 'rb') as f:
            while True:
                chunk = f.read(chunk_size)
                if not chunk:
                    break
                hash_func.update(chunk)
                bytes_processed += len(chunk)
                
                # å¯¹äºè¶…å¤§æ–‡ä»¶ï¼Œå¯ä»¥è€ƒè™‘åªè®¡ç®—éƒ¨åˆ†å†…å®¹çš„å“ˆå¸Œ
                if file_size > 200 * 1024 * 1024 and bytes_processed > 50 * 1024 * 1024:
                    logger.info(f"å¤§æ–‡ä»¶é‡‡ç”¨éƒ¨åˆ†å“ˆå¸Œ: {image_path}")
                    break
        
        return hash_func.hexdigest()
        
    except (IOError, OSError) as e:
        logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {image_path}, é”™è¯¯: {e}")
        return None
    except MemoryError:
        logger.error(f"å†…å­˜ä¸è¶³ï¼Œæ— æ³•å¤„ç†æ–‡ä»¶: {image_path}")
        return None
    except Exception as e:
        logger.error(f"å“ˆå¸Œè®¡ç®—æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {image_path}, é”™è¯¯: {e}")
        return None
def get_image_size(image_path):
    """ä¼˜åŒ–çš„å›¾ç‰‡å°ºå¯¸è·å–å‡½æ•°ï¼Œå¤„ç†å¤§æ–‡ä»¶"""
    try:
        normalized_path = normalize_path(image_path)
        
        if not safe_file_exists(normalized_path):
            return None
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œé¿å…åŠ è½½è¶…å¤§æ–‡ä»¶
        file_size = safe_file_size(normalized_path)
        if file_size > 200 * 1024 * 1024:  # 200MB
            logger.warning(f"å›¾ç‰‡æ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡å°ºå¯¸æ£€æµ‹: {image_path} ({file_size/1024/1024:.1f}MB)")
            return None
            
        with Image.open(normalized_path) as img:
            size = img.size
            # éªŒè¯å°ºå¯¸åˆç†æ€§
            if size[0] <= 0 or size[1] <= 0 or size[0] > 100000 or size[1] > 100000:
                logger.warning(f"å›¾ç‰‡å°ºå¯¸å¼‚å¸¸: {image_path}, å°ºå¯¸: {size}")
                return None
            return size
            
    except (IOError, OSError, UnidentifiedImageError):
        return None
    except MemoryError:
        logger.error(f"å†…å­˜ä¸è¶³ï¼Œæ— æ³•è¯»å–å›¾ç‰‡å°ºå¯¸: {image_path}")
        return None
    except Exception as e:
        logger.warning(f"è¯»å–å›¾ç‰‡å°ºå¯¸æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {image_path}, é”™è¯¯: {e}")
        return None

def check_file_content_samples(files_info):
    """
    å¯¹å°æ–‡ä»¶è¿›è¡Œå†…å®¹é‡‡æ ·æ¯”è¾ƒï¼Œæ£€æµ‹å“ˆå¸Œå†²çª
    """
    if len(files_info) < 2:
        return False
    
    try:
        # è¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºåŸºå‡†
        base_file = files_info[0]['path']
        if not safe_file_exists(base_file):
            return False
            
        with open(normalize_path(base_file), 'rb') as f:
            base_content = f.read()
        
        # æ¯”è¾ƒå…¶ä»–æ–‡ä»¶
        for file_info in files_info[1:]:
            file_path = file_info['path']
            if not safe_file_exists(file_path):
                continue
                
            try:
                with open(normalize_path(file_path), 'rb') as f:
                    content = f.read()
                
                if content != base_content:
                    return True  # å‘ç°å†…å®¹ä¸åŒï¼Œç¡®è®¤å“ˆå¸Œå†²çª
                    
            except Exception as e:
                logger.warning(f"è¯»å–æ–‡ä»¶ç”¨äºå†²çªæ£€æµ‹å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                continue
        
        return False  # æ‰€æœ‰æ–‡ä»¶å†…å®¹ç›¸åŒ
        
    except Exception as e:
        logger.warning(f"å“ˆå¸Œå†²çªæ£€æµ‹å¤±è´¥: {e}")
        return False
def detect_potential_hash_collision(file_groups, sample_check=True):
    """
    æ£€æµ‹æ½œåœ¨çš„å“ˆå¸Œå†²çª
    sample_check: æ˜¯å¦å¯¹å°æ–‡ä»¶è¿›è¡Œå†…å®¹é‡‡æ ·æ¯”è¾ƒ
    """
    collision_suspects = []
    
    for group_files in file_groups:
        if len(group_files) < 2:
            continue
            
        # æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„ï¼Œå¤§å°ä¸åŒçš„æ–‡ä»¶è‚¯å®šä¸æ˜¯é‡å¤çš„
        size_groups = {}
        for file_info in group_files:
            file_size = file_info.get('size', 0)
            if file_size not in size_groups:
                size_groups[file_size] = []
            size_groups[file_size].append(file_info)
        
        # æ£€æŸ¥æ¯ä¸ªå¤§å°ç»„
        for size, files_of_same_size in size_groups.items():
            if len(files_of_same_size) < 2:
                continue
                
            # å¦‚æœåŒä¸€å“ˆå¸Œå€¼ä¸‹æœ‰å¤šä¸ªä¸åŒå¤§å°çš„æ–‡ä»¶ï¼Œè¿™å¾ˆå¯èƒ½æ˜¯å“ˆå¸Œå†²çª
            if len(size_groups) > 1:
                logger.warning(f"å‘ç°å¯ç–‘å“ˆå¸Œå†²çªï¼šç›¸åŒå“ˆå¸Œä½†ä¸åŒæ–‡ä»¶å¤§å°")
                collision_suspects.extend(group_files)
                continue
            
            # å¯¹äºå°æ–‡ä»¶ï¼ˆ<5MBï¼‰ï¼Œè¿›è¡Œå†…å®¹é‡‡æ ·æ¯”è¾ƒ
            if sample_check and size < 5 * 1024 * 1024 and len(files_of_same_size) > 1:
                if check_file_content_samples(files_of_same_size):
                    logger.warning(f"å‘ç°ç¡®è®¤çš„å“ˆå¸Œå†²çªï¼š{[f['path'] for f in files_of_same_size]}")
                    collision_suspects.extend(files_of_same_size)
    
    return collision_suspects
def detect_supplement_hash_collision(added_images, skipped_images, main_hashes):
    """
    æ£€æµ‹å¢è¡¥æ¨¡å¼ä¸­çš„å“ˆå¸Œå†²çª
    ä¸»è¦æ£€æµ‹è¡¥å……æ–‡ä»¶å¤¹å†…éƒ¨çš„å“ˆå¸Œå†²çª
    """
    collision_suspects = []
    
    # æ”¶é›†æ‰€æœ‰è¡¥å……æ–‡ä»¶æŒ‰å“ˆå¸Œåˆ†ç»„
    hash_groups = {}
    all_supplement_files = added_images + skipped_images
    
    for file_info in all_supplement_files:
        file_hash = file_info.get('hash')
        if file_hash:
            hash_groups.setdefault(file_hash, []).append(file_info)
    
    # æ‰¾å‡ºæœ‰å¤šä¸ªæ–‡ä»¶çš„å“ˆå¸Œç»„
    potential_collision_groups = []
    for hash_value, files in hash_groups.items():
        if len(files) > 1:
            potential_collision_groups.append(files)
    
    # å¤ç”¨ä¹‹å‰å®šä¹‰çš„æ£€æµ‹å‡½æ•°
    if potential_collision_groups:
        collision_suspects = detect_potential_hash_collision(potential_collision_groups, sample_check=True)
    
    return collision_suspects

def safe_multiprocess_operation(func, items, max_workers=None):
    """
    å®‰å…¨çš„å¤šè¿›ç¨‹æ“ä½œï¼ŒåŒ…å«å®Œå–„çš„é”™è¯¯å¤„ç†å’Œèµ„æºç®¡ç†
    """
    if not items:
        return []
    
    if max_workers is None:
        max_workers = min(cpu_count(), len(items), 8)  # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°é¿å…èµ„æºè¿‡åº¦æ¶ˆè€—
    
    # å¦‚æœä»»åŠ¡æ•°é‡å¾ˆå°‘ï¼Œç›´æ¥å•è¿›ç¨‹å¤„ç†
    if len(items) < max_workers * 2:
        logger.info("ä»»åŠ¡æ•°é‡è¾ƒå°‘ï¼Œä½¿ç”¨å•è¿›ç¨‹å¤„ç†")
        return [func(item) for item in items]
    
    pool = None
    try:
        logger.info(f"ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¤„ç† {len(items)} ä¸ªä»»åŠ¡")
        
        # åˆ›å»ºè¿›ç¨‹æ± 
        pool = Pool(max_workers)
        
        # ä½¿ç”¨map_asyncä»¥ä¾¿å¯ä»¥è®¾ç½®è¶…æ—¶å’Œæ›´å¥½çš„é”™è¯¯å¤„ç†
        result = pool.map_async(func, items)
        
        # ç­‰å¾…ç»“æœï¼Œè®¾ç½®ä¸€ä¸ªåˆç†çš„è¶…æ—¶æ—¶é—´
        timeout = max(300, len(items) * 2)  # æœ€å°‘5åˆ†é’Ÿï¼Œæˆ–æ¯ä¸ªä»»åŠ¡2ç§’
        results = result.get(timeout=timeout)
        
        return results
        
    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·ä¸­æ–­äº†å¤šè¿›ç¨‹æ“ä½œ")
        if pool:
            pool.terminate()
            pool.join()
        raise
    except Exception as e:
        logger.error(f"å¤šè¿›ç¨‹æ“ä½œå¤±è´¥: {e}")
        if pool:
            pool.terminate()
            pool.join()
        
        # å›é€€åˆ°å•è¿›ç¨‹å¤„ç†
        logger.info("å›é€€åˆ°å•è¿›ç¨‹å¤„ç†")
        results = []
        for i, item in enumerate(items):
            try:
                result = func(item)
                results.append(result)
                
                # æ¯å¤„ç†100ä¸ªé¡¹ç›®æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
                if (i + 1) % 100 == 0:
                    logger.info(f"å•è¿›ç¨‹å¤„ç†è¿›åº¦: {i+1}/{len(items)}")
                    
            except Exception as item_error:
                logger.error(f"å¤„ç†é¡¹ç›®å¤±è´¥: {item}, é”™è¯¯: {item_error}")
                results.append(None)
        
        return results
    finally:
        # ç¡®ä¿è¿›ç¨‹æ± è¢«æ­£ç¡®å…³é—­
        if pool:
            try:
                pool.close()
                pool.join()
            except Exception as e:
                logger.error(f"å…³é—­è¿›ç¨‹æ± æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def normalize_path(path):
    """
    æ ‡å‡†åŒ–è·¯å¾„å¤„ç†ï¼Œè§£å†³Unicodeç­‰ç¼–ç é—®é¢˜
    """
    try:
        # é¦–å…ˆå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆå¤„ç†Pathå¯¹è±¡ï¼‰
        if isinstance(path, Path):
            path = str(path)
        # å¤„ç†Unicodeè·¯å¾„
        if isinstance(path, bytes):
            # å¦‚æœæ˜¯å­—èŠ‚ä¸²ï¼Œå°è¯•è§£ç 
            try:
                path = path.decode('utf-8')
            except UnicodeDecodeError:
                path = path.decode('gbk', errors='ignore')  # Windowsä¸­æ–‡ç³»ç»Ÿå¤‡é€‰
        
        # æ ‡å‡†åŒ–è·¯å¾„
        normalized = os.path.normpath(os.path.abspath(path))
        
        # åœ¨Windowsä¸Šå¤„ç†é•¿è·¯å¾„é—®é¢˜
        if sys.platform == 'win32' and len(normalized) > 260:
            if not normalized.startswith('\\\\?\\'):
                normalized = '\\\\?\\' + normalized
        
        return normalized
        
    except (UnicodeError, TypeError, OSError) as e:
        logger.warning(f"è·¯å¾„æ ‡å‡†åŒ–å¤±è´¥: {path}, é”™è¯¯: {e}")
        # å›é€€åˆ°åŸå§‹è·¯å¾„
        return str(path) if path else ''

def safe_file_exists(path):
    """å®‰å…¨çš„æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥ï¼Œå¤„ç†Unicodeè·¯å¾„"""
    try:
        normalized_path = normalize_path(path)
        return os.path.exists(normalized_path)
    except (OSError, UnicodeError, TypeError):
        # å¦‚æœæ ‡å‡†åŒ–å¤±è´¥ï¼Œå°è¯•ç›´æ¥æ£€æŸ¥åŸè·¯å¾„
        try:
            return os.path.exists(path)
        except:
            return False
def safe_file_size(path):
    """å®‰å…¨çš„æ–‡ä»¶å¤§å°è·å–ï¼Œå¤„ç†Unicodeè·¯å¾„"""
    try:
        normalized_path = normalize_path(path)
        return os.path.getsize(normalized_path)
    except (OSError, UnicodeError, TypeError):
        try:
            return os.path.getsize(path)
        except:
            return 0
def safe_walk_directory(folder):
    """å®‰å…¨çš„ç›®å½•éå†ï¼Œå¤„ç†Unicodeæ–‡ä»¶å"""
    files_found = []
    folder = normalize_path(folder)
    try:
        for root, dirs, files in os.walk(folder):
            # å¤„ç†æ–‡ä»¶åç¼–ç é—®é¢˜
            for file in files:
                try:
                    file_path = os.path.join(root, file)
                    # éªŒè¯è·¯å¾„æ˜¯å¦å¯ç”¨
                    if safe_file_exists(file_path):
                        files_found.append(normalize_path(file_path))
                except (UnicodeError, OSError) as e:
                    logger.warning(f"è·³è¿‡æœ‰é—®é¢˜çš„æ–‡ä»¶: {file}, é”™è¯¯: {e}")
                    continue
    except (OSError, UnicodeError) as e:
        logger.error(f"éå†ç›®å½•å¤±è´¥: {folder}, é”™è¯¯: {e}")
    
    return files_found

# æ·»åŠ ä¿¡å·å¤„ç†å™¨æ¥ä¼˜é›…åœ°å¤„ç†ä¸­æ–­
def signal_handler(signum, frame):
    logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†èµ„æº...")
    sys.exit(0)

# åœ¨æ¨¡å—åŠ è½½æ—¶æ³¨å†Œä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

def collect_images(folder, exts=None):
    """
    é€’å½’æ”¶é›†æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶è·¯å¾„ã€å¤§å°ã€å°ºå¯¸ã€‚
    è¿”å›ï¼š[{path, size, shape}...]
    ä½¿ç”¨æ”¹è¿›çš„å¤šè¿›ç¨‹å¤„ç†æ¥æ”¶é›†å›¾ç‰‡
    ä½¿ç”¨å®‰å…¨è·¯å¾„å¤„ç†çš„å›¾ç‰‡æ”¶é›†å‡½æ•°
    """
    if exts is None:
        exts = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff', '.avif'}
    # ä½¿ç”¨å®‰å…¨çš„ç›®å½•éå†
    all_files = safe_walk_directory(folder)
    # è¿‡æ»¤å›¾ç‰‡æ–‡ä»¶
    image_files = []
    for file_path in all_files:
        try:
            ext = os.path.splitext(file_path)[1].lower()
            if ext in exts:
                image_files.append(file_path)
        except Exception as e:
            logger.warning(f"å¤„ç†æ–‡ä»¶æ‰©å±•åå¤±è´¥: {file_path}, é”™è¯¯: {e}")
            continue
    
    logger.info(f"å…±å‘ç°å›¾ç‰‡æ–‡ä»¶ {len(image_files)} å¼ ")

    # ä½¿ç”¨å®‰å…¨çš„å¤šè¿›ç¨‹æ“ä½œ
    sizes = safe_multiprocess_operation(get_image_size, image_files)
    
    image_meta = []
    for path, shape in zip(image_files, sizes):
        try:
            size = safe_file_size(path)
            if size > 0:  # åªåŒ…å«æœ‰æ•ˆå¤§å°çš„æ–‡ä»¶
                image_meta.append({'path': path, 'size': size, 'shape': shape})
        except Exception as e:
            logger.warning(f"å¤„ç†å›¾ç‰‡å…ƒæ•°æ®å¤±è´¥: {path}, é”™è¯¯: {e}")
            continue
    
    logger.info(f"æˆåŠŸè¯»å–å…ƒæ•°æ®å›¾ç‰‡æ•°: {len(image_meta)}")
    return image_meta

def collect_videos(folder, exts=None):
    """
    é€’å½’æ”¶é›†æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰è§†é¢‘æ–‡ä»¶è·¯å¾„ã€å¤§å°ã€æ–‡ä»¶åã€‚
    è¿”å›ï¼š[{path, size, name}...]
    """
    if exts is None:
        exts = {'.mp4', '.mov'}
    video_files = []
    for root, _, files in os.walk(folder):
        for file in files:
            if os.path.splitext(file)[1].lower() in exts:
                video_files.append(os.path.join(root, file))
    logger.info(f"å…±å‘ç°è§†é¢‘æ–‡ä»¶ {len(video_files)} ä¸ª")
    video_meta = []
    for path in video_files:
        try:
            size = os.path.getsize(path)
            name = os.path.basename(path)
        except Exception as e:
            logger.warning(f"æ— æ³•è¯»å–è§†é¢‘æ–‡ä»¶ä¿¡æ¯: {path}, é”™è¯¯: {e}")
            continue
        video_meta.append({'path': path, 'size': size, 'name': name})
    logger.info(f"æˆåŠŸè¯»å–å…ƒæ•°æ®è§†é¢‘æ•°: {len(video_meta)}")
    return video_meta
def _hash_worker(args):
    path, method = args
    try:
        return path, get_image_hash(path, method)
    except Exception as e:
        logger.error(f"å“ˆå¸Œè®¡ç®—å¤±è´¥: {path}, é”™è¯¯: {e}")
        return path, None

def is_valid_image(image_path):
    """
    æ”¹è¿›çš„å›¾ç‰‡éªŒè¯æ–¹æ³•ï¼Œä½¿ç”¨load()ä»£æ›¿verify()
    """
    try:
        with Image.open(image_path) as img:
            img.load()  # ä½¿ç”¨load()ä»£æ›¿verify()ï¼Œä¸ä¼šç ´åImageå¯¹è±¡
            # æ£€æŸ¥å›¾ç‰‡åŸºæœ¬å±æ€§
            if img.size[0] <= 0 or img.size[1] <= 0:
                return False
            return True
    except (IOError, OSError, UnidentifiedImageError):
        return False
    except Exception as e:
        logger.warning(f"å›¾ç‰‡éªŒè¯æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {image_path}, é”™è¯¯: {e}")
        return False
    
def find_duplicates(folder, report_path, hash_method='md5', dry_run=False, log_callback=None, progress_callback=None):
    """
    å»é‡æ¨¡å¼ä¸»æµç¨‹ï¼šæŸ¥æ‰¾é‡å¤å›¾ç‰‡å’Œè§†é¢‘å¹¶è¾“å‡ºæŠ¥å‘Šã€‚
    """
    log = []
    corrupt_files = []
    
    def log_emit(msg):
        log.append(msg)
        if log_callback:
            log_callback(msg)
    
    def progress_emit(value):
        if progress_callback:
            progress_callback(value)
    
    if dry_run:
        log_emit(tr('dry_run'))
    
    # æ”¶é›†å›¾ç‰‡ä¿¡æ¯
    log_emit(tr('scanning_images'))
    image_meta = collect_images(folder)
    total_images_scanned = len(image_meta)
    log_emit(tr('images_found', count=total_images_scanned))
    progress_emit(0.1)
    
    # æŒ‰å¤§å°å’Œå°ºå¯¸åˆ†ç»„
    group_map = {}
    for meta in image_meta:
        if meta['shape'] is None:  # è·³è¿‡æ— æ³•è¯»å–å°ºå¯¸çš„å›¾ç‰‡
            continue
        key = (meta['size'], meta['shape'])
        group_map.setdefault(key, []).append(meta)
    
    groups_to_process = sum(1 for files in group_map.values() if len(files) >= 2)
    if groups_to_process > 0:
        log_emit(tr('analyzing_duplicates', count=groups_to_process))
    progress_emit(0.2)
    
    # è®¡ç®—å“ˆå¸Œå€¼å¹¶æ‰¾å‡ºé‡å¤ç»„
    img_groups = []
    processed_groups = 0
    
    for idx, (key, files) in enumerate(group_map.items()):
        if len(files) < 2:
            processed_groups += 1
            continue
            
        file_details = []
        for meta in files:
            try:
                file_hash = get_image_hash(meta['path'], hash_method)
                if file_hash is None:
                    corrupt_files.append(meta['path'])
                    continue
                    
                file_info = {
                    'path': meta['path'],
                    'size': meta['size'],
                    'shape': meta['shape'],
                    'hash': file_hash,
                    'mtime': os.path.getmtime(meta['path']) if os.path.exists(meta['path']) else 0,
                    'is_corrupt': False
                }
                
                # ä½¿ç”¨æ”¹è¿›çš„å›¾ç‰‡éªŒè¯æ–¹æ³•
                if not is_valid_image(meta['path']):
                    file_info['is_corrupt'] = True
                    corrupt_files.append(meta['path'])
                
                file_details.append(file_info)
                
            except Exception as e:
                corrupt_files.append(meta['path'])
        
        # æŒ‰å“ˆå¸Œå€¼åˆ†ç»„
        hash_groups = {}
        for file_info in file_details:
            hash_groups.setdefault(file_info['hash'], []).append(file_info)
        
        # åªä¿ç•™æœ‰é‡å¤çš„ç»„
        for hash_val, group in hash_groups.items():
            if len(group) > 1:
                img_groups.append(group)
        
        processed_groups += 1
        progress = 0.2 + 0.5 * (processed_groups / len(group_map))
        # progress_emit(progress)
    
    # ğŸ”¥ åœ¨è¿™é‡Œæ·»åŠ å“ˆå¸Œå†²çªæ£€æµ‹ ğŸ”¥
    if img_groups:  # åªæœ‰å½“æœ‰é‡å¤ç»„æ—¶æ‰æ£€æµ‹
        log_emit("æ­£åœ¨æ£€æµ‹æ½œåœ¨çš„å“ˆå¸Œå†²çª...")
        collision_suspects = detect_potential_hash_collision(img_groups)
        if collision_suspects:
            logger.warning(f"å‘ç° {len(collision_suspects)} ä¸ªå¯ç–‘çš„å“ˆå¸Œå†²çªæ–‡ä»¶")
            log_emit(f"âš ï¸ å‘ç° {len(collision_suspects)} ä¸ªå¯ç–‘çš„å“ˆå¸Œå†²çªæ–‡ä»¶ï¼Œå»ºè®®æ‰‹åŠ¨æ£€æŸ¥")

    progress_emit(0.7)
    
    # å¤„ç†è§†é¢‘æ–‡ä»¶
    log_emit(tr('scanning_videos'))
    video_meta = collect_videos(folder)
    total_videos_scanned = len(video_meta)
    log_emit(tr('videos_found', count=total_videos_scanned))
    vid_groups = []
    
    if video_meta:
        # æŒ‰æ–‡ä»¶åå’Œå¤§å°åˆ†ç»„
        video_group_map = {}
        for meta in video_meta:
            key = (meta['name'], meta['size'])
            video_group_map.setdefault(key, []).append(meta)
        
        for group in video_group_map.values():
            if len(group) > 1:
                # æ„å»ºè§†é¢‘æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
                video_group = []
                for meta in group:
                    video_info = {
                        'path': meta['path'],
                        'name': meta['name'],
                        'size': meta['size'],
                        'mtime': os.path.getmtime(meta['path']) if os.path.exists(meta['path']) else 0,
                        'is_corrupt': False
                    }
                    video_group.append(video_info)
                vid_groups.append(video_group)
    
    progress_emit(0.9)
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    total_img_files = sum(len(group) for group in img_groups)
    total_vid_files = sum(len(group) for group in vid_groups)
    
    stats = {
        'total_img_groups': len(img_groups),
        'total_img_files': total_img_files,
        'total_vid_groups': len(vid_groups), 
        'total_vid_files': total_vid_files,
        'total_images_scanned': total_images_scanned,
        'total_videos_scanned': total_videos_scanned,
        'corrupt_files_count': len(corrupt_files),
        'potential_space_saved': sum(
            sum(file_info['size'] for file_info in group[1:]) 
            for group in img_groups
        ) + sum(
            sum(file_info['size'] for file_info in group[1:])
            for group in vid_groups
        )
    }
    
    # å†™æŠ¥å‘Šæ–‡ä»¶ (ä¿æŒå…¼å®¹æ€§)
    _write_dedup_report(report_path, img_groups, vid_groups, stats)
    
    log_emit(tr('analysis_complete'))
    progress_emit(1.0)
    
    return {
        'img_groups': img_groups,
        'vid_groups': vid_groups,
        'stats': stats,
        'log': log,
        'progress': 1.0,
        'corrupt_files': corrupt_files
    }

def supplement_duplicates(main_folder, supplement_folder, report_path, hash_method='md5', dry_run=False, log_callback=None, progress_callback=None):
    """
    å¢è¡¥æ¨¡å¼ä¸»æµç¨‹ï¼šè¡¥å……å›¾ç‰‡å’Œè§†é¢‘å¹¶è¾“å‡ºæŠ¥å‘Šã€‚
    è¿”å›dict: {
        'added_images': List[dict],  # éœ€è¦å¢è¡¥çš„å›¾ç‰‡è¯¦ç»†ä¿¡æ¯
        'skipped_images': List[dict],  # å·²å­˜åœ¨çš„å›¾ç‰‡
        'added_videos': List[dict],  # éœ€è¦å¢è¡¥çš„è§†é¢‘
        'skipped_videos': List[dict],  # å·²å­˜åœ¨çš„è§†é¢‘
        'target_dirs': dict,  # ç›®æ ‡ç›®å½•
        'stats': dict,  # ç»Ÿè®¡ä¿¡æ¯
        'log': List[str],
        'progress': float,
        'corrupt_files': List[str]
    }
    """
    log = []
    corrupt_files = []
    def log_emit(msg):
        log.append(msg)
        if log_callback:
            log_callback(msg)
    
    def progress_emit(value):
        if progress_callback:
            progress_callback(value)
    
    if dry_run:
        log_emit(tr('dry_run'))
    
    # æ‰«æä¸»æ–‡ä»¶å¤¹
    main_meta = collect_images(main_folder)
    progress_emit(0.2)
    
    # æ‰«æè¡¥å……æ–‡ä»¶å¤¹  
    supplement_meta = collect_images(supplement_folder)
    progress_emit(0.3)
    
    log_emit(tr('main_img_count', main=len(main_meta), supp=len(supplement_meta)))
    
    # æ„å»ºä¸»æ–‡ä»¶å¤¹å“ˆå¸Œé›†åˆ
    main_hashes = set()
    for idx, meta in enumerate(main_meta):
        try:
            file_hash = get_image_hash(meta['path'], hash_method)
            if file_hash:
                main_hashes.add(file_hash)
        except Exception as e:
            log_emit(tr('hash_fail', path=meta['path'], err=e))
        
        # ğŸ”¥ ä¼˜åŒ–è¿›åº¦æ›´æ–°ï¼šç¡®ä¿å³ä½¿æ–‡ä»¶å°‘ä¹Ÿæœ‰è¿›åº¦åé¦ˆ
        if len(main_meta) > 0:
            progress = 0.3 + 0.3 * ((idx + 1) / len(main_meta))
            progress_emit(progress)
    
    log_emit(tr('main_hash_done', count=len(main_hashes)))
    progress_emit(0.6)
    
    # å¤„ç†è¡¥å……æ–‡ä»¶å¤¹çš„å›¾ç‰‡
    added_images = []
    skipped_images = []
    
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    supplement_dir = os.path.join(main_folder, tr('supp_dir', timestamp=timestamp))
    
    for idx, meta in enumerate(supplement_meta):
        try:
            file_hash = get_image_hash(meta['path'], hash_method)
            if not file_hash:
                log_emit(tr('supp_hash_fail', path=meta['path']))
                corrupt_files.append(meta['path'])
                continue
            
            # éªŒè¯å›¾ç‰‡æ˜¯å¦æŸå
            is_corrupt = False
            try:
                with Image.open(meta['path']) as img:
                    img.verify()
            except Exception:
                is_corrupt = True
                corrupt_files.append(meta['path'])
            
            base_name = os.path.basename(meta['path'])
            target_path = os.path.join(supplement_dir, base_name)
            
            file_info = {
                'path': meta['path'],
                'target_path': target_path,
                'size': meta['size'],
                'shape': meta['shape'],
                'hash': file_hash,
                'mtime': os.path.getmtime(meta['path']) if os.path.exists(meta['path']) else 0,
                'is_corrupt': is_corrupt
            }
            
            if file_hash in main_hashes:
                skipped_images.append(file_info)
                log_emit(tr('supp_exists', path=meta['path']))
            else:
                added_images.append(file_info)
        
        except Exception as e:
            log_emit(tr('hash_fail', path=meta['path'], err=e))
            corrupt_files.append(meta['path'])
        
        # ğŸ”¥ ä¼˜åŒ–è¿›åº¦æ›´æ–°ï¼šæ¯ä¸ªæ–‡ä»¶éƒ½æ›´æ–°è¿›åº¦
        if len(supplement_meta) > 0:
            progress = 0.6 + 0.15 * ((idx + 1) / len(supplement_meta))  # ç»™åç»­æ­¥éª¤ç•™å‡ºè¿›åº¦ç©ºé—´
            # progress_emit(progress)
    
    # ğŸ”¥ åœ¨è¿™é‡Œæ·»åŠ å¢è¡¥æ¨¡å¼çš„å“ˆå¸Œå†²çªæ£€æµ‹ ğŸ”¥
    log_emit("æ­£åœ¨æ£€æµ‹è¡¥å……æ–‡ä»¶çš„å“ˆå¸Œå†²çª...")
    progress_emit(0.75)  # è¡¥å……æ–‡ä»¶å¤„ç†å®Œæˆ
    collision_suspects = detect_supplement_hash_collision(added_images, skipped_images, main_hashes)
    if collision_suspects:
        logger.warning(f"åœ¨è¡¥å……æ–‡ä»¶ä¸­å‘ç° {len(collision_suspects)} ä¸ªå¯ç–‘çš„å“ˆå¸Œå†²çªæ–‡ä»¶")
        log_emit(f"âš ï¸ åœ¨è¡¥å……æ–‡ä»¶ä¸­å‘ç° {len(collision_suspects)} ä¸ªå¯ç–‘çš„å“ˆå¸Œå†²çªæ–‡ä»¶ï¼Œå»ºè®®æ‰‹åŠ¨æ£€æŸ¥")

    progress_emit(0.80)
    
    # å¤„ç†è§†é¢‘æ–‡ä»¶
    log_emit("æ­£åœ¨å¤„ç†è§†é¢‘æ–‡ä»¶...")
    main_videos = collect_videos(main_folder)
    supplement_videos = collect_videos(supplement_folder)
    
    main_video_keys = set((v['name'], v['size']) for v in main_videos)
    mp4_dir = os.path.join(main_folder, f'MP4_{timestamp}')
    
    added_videos = []
    skipped_videos = []
    
    for meta in supplement_videos:
        key = (meta['name'], meta['size'])
        target_path = os.path.join(mp4_dir, meta['name'])
        
        video_info = {
            'path': meta['path'],
            'target_path': target_path,
            'name': meta['name'],
            'size': meta['size'],
            'mtime': os.path.getmtime(meta['path']) if os.path.exists(meta['path']) else 0,
            'is_corrupt': False
        }
        
        if key in main_video_keys:
            skipped_videos.append(video_info)
            log_emit(tr('vid_supp_exists', path=meta['path']))
        else:
            added_videos.append(video_info)
        # ğŸ”¥ ä¸ºè§†é¢‘å¤„ç†æ·»åŠ è¿›åº¦æ›´æ–°
        if len(supplement_videos) > 0:
            progress = 0.80 + 0.1 * ((idx + 1) / len(supplement_videos))
            # progress_emit(progress)

    progress_emit(0.90)  # è§†é¢‘å¤„ç†å®Œæˆ
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_add_size = sum(img['size'] for img in added_images) + sum(vid['size'] for vid in added_videos)
    # ğŸ”¥ æŠ¥å‘Šå†™å…¥é˜¶æ®µ
    log_emit("æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
    progress_emit(0.95)

    stats = {
        'main_scanned': len(main_meta) + len(main_videos),  # ğŸ”¥ åŠ ä¸Šè§†é¢‘æ•°é‡
        'supplement_scanned': len(supplement_meta) + len(supplement_videos),  # ğŸ”¥ åŠ ä¸Šè§†é¢‘æ•°é‡
        'images_to_add': len(added_images),
        'images_skipped': len(skipped_images),
        'videos_to_add': len(added_videos),
        'videos_skipped': len(skipped_videos),
        'total_add_size': total_add_size,
        'corrupt_files_count': len(corrupt_files)
    }
    
    target_dirs = {
        'supplement_dir': supplement_dir,
        'mp4_dir': mp4_dir
    }
    
    # å†™æŠ¥å‘Šæ–‡ä»¶ (ä¿æŒå…¼å®¹æ€§)
    _write_supplement_report(report_path, added_images, skipped_images, added_videos, skipped_videos, target_dirs, stats, dry_run, corrupt_files)
    
    log_emit(tr('dedup_done', path=report_path))
    progress_emit(1.0)  # æœ€ç»ˆå®Œæˆ
    
    return {
        'added_images': added_images,
        'skipped_images': skipped_images,
        'added_videos': added_videos,
        'skipped_videos': skipped_videos,
        'target_dirs': target_dirs,
        'stats': stats,
        'log': log,
        'progress': 1.0,
        'corrupt_files': corrupt_files
    }

def _write_dedup_report(report_path, img_groups, vid_groups, stats):
    """å†™å…¥å»é‡æŠ¥å‘Šæ–‡ä»¶"""
    with open(report_path, 'w', encoding='utf-8') as f:
        if LANG == 'zh':
            f.write('å»é‡å›¾ç‰‡æŠ¥å‘Š\n\n')
            f.write(f'å…±æ£€æµ‹åˆ°{stats["total_img_groups"]}ç»„é‡å¤å›¾ç‰‡ï¼Œå…±{stats["total_img_files"]}å¼ å›¾ç‰‡\n\n')
        else:
            f.write('Deduplication Report\n\n')
            f.write(f'{stats["total_img_groups"]} duplicate image groups, {stats["total_img_files"]} images in total\n\n')
        
        if img_groups:
            for group_id, group in enumerate(img_groups, 1):
                f.write(f'é‡å¤å›¾ç‰‡ç»„{group_id} (å“ˆå¸Œ: {group[0]["hash"]}):\n' if LANG == 'zh' else f'Duplicate Image Group {group_id} (hash: {group[0]["hash"]}):\n')
                for file_info in group:
                    f.write(f"    {file_info['path']}\n")
                f.write("\n")
        else:
            f.write('æœªå‘ç°é‡å¤å›¾ç‰‡\n\n' if LANG == 'zh' else 'No duplicate images found\n\n')
        
        if LANG == 'zh':
            f.write(f'å…±æ£€æµ‹åˆ°{stats["total_vid_groups"]}ç»„é‡å¤è§†é¢‘ï¼Œå…±{stats["total_vid_files"]}ä¸ªè§†é¢‘\n\n')
        else:
            f.write(f'{stats["total_vid_groups"]} duplicate video groups, {stats["total_vid_files"]} videos in total\n\n')
        
        if vid_groups:
            for idx, group in enumerate(vid_groups, 1):
                f.write(f'è§†é¢‘é‡å¤ç»„{idx}:\n' if LANG == 'zh' else f'Duplicate Video Group {idx}:\n')
                for file_info in group:
                    f.write(f"    {file_info['path']}\n")
                f.write("\n")
        else:
            f.write('æœªå‘ç°é‡å¤è§†é¢‘\n' if LANG == 'zh' else 'No duplicate videos found\n')

def _write_supplement_report(report_path, added_images, skipped_images, added_videos, skipped_videos, target_dirs, stats, dry_run, corrupt_files=None):
    """å†™å…¥å¢è¡¥æŠ¥å‘Šæ–‡ä»¶"""
    with open(report_path, 'w', encoding='utf-8') as f:
        if dry_run:
            f.write(tr('dry_run') + '\n\n')
        f.write(tr('supp_report') + '\n\n')
        f.write(tr('supp_img_success', count=len(added_images), dir=target_dirs['supplement_dir']) + '\n')
        
        for img in added_images:
            f.write(f"    {img['path']}\n")
        
        f.write(tr('supp_img_exists', count=len(skipped_images)) + '\n')
        for img in skipped_images:
            f.write(f"    {img['path']}\n")
        
        f.write(tr('supp_vid_success', count=len(added_videos), dir=target_dirs['mp4_dir']) + '\n')
        for vid in added_videos:
            f.write(f"    {vid['path']}\n")
        
        f.write(tr('supp_vid_exists', count=len(skipped_videos)) + '\n')
        for vid in skipped_videos:
            f.write(f"    {vid['path']}\n")
        
                # ğŸ”¥ æ·»åŠ æŸåæ–‡ä»¶è¯¦ç»†ä¿¡æ¯
        if corrupt_files and len(corrupt_files) > 0:
            f.write(f"\nâŒ å‘ç° {len(corrupt_files)} ä¸ªæŸåæˆ–æ— æ³•å¤„ç†çš„æ–‡ä»¶ï¼š\n")
            f.write("=" * 60 + "\n")
            for idx, corrupt_file in enumerate(corrupt_files, 1):
                f.write(f"{idx:3d}. {corrupt_file}\n")
            f.write("\nå»ºè®®ï¼šè¯·æ£€æŸ¥è¿™äº›æ–‡ä»¶æ˜¯å¦ç¡®å®æŸåï¼Œå¦‚æœç¡®è®¤æŸåè¯·åˆ é™¤æˆ–ä¿®å¤ã€‚\n\n")